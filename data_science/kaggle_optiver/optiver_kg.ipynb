{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import catboost as cbt \n",
    "from numba import njit, prange\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "from catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         stock_return[i] \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mlog(wap[i] \u001b[39m/\u001b[39m wap[i \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m10_000\u001b[39m) \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m6\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m stock_return\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m df_train \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mgroupby([\u001b[39m\"\u001b[39;49m\u001b[39mstock_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdate_id\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m frame: frame\u001b[39m.\u001b[39;49massign(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     stock_return\u001b[39m=\u001b[39;49mcalculate_stock_return(frame[\u001b[39m\"\u001b[39;49m\u001b[39mwap\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m ))\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m df_train[\u001b[39m'\u001b[39m\u001b[39mindex_return\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdf_train[\u001b[39m\"\u001b[39m\u001b[39mstock_return\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m df_train[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/miller/data_science/kaggle_optiver/optiver_kg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m df_train \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39mdropna(subset\u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mindex_return\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1353\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1352\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1353\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[1;32m   1354\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1355\u001b[0m         \u001b[39m# gh-20949\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m         \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m         \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_apply_general(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1406\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m     not_indexed_same \u001b[39m=\u001b[39m mutated\n\u001b[0;32m-> 1406\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_applied_output(\n\u001b[1;32m   1407\u001b[0m     data,\n\u001b[1;32m   1408\u001b[0m     values,\n\u001b[1;32m   1409\u001b[0m     not_indexed_same,\n\u001b[1;32m   1410\u001b[0m     is_transform,\n\u001b[1;32m   1411\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1419\u001b[0m, in \u001b[0;36mDataFrameGroupBy._wrap_applied_output\u001b[0;34m(self, data, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_constructor()\n\u001b[1;32m   1418\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(first_not_none, DataFrame):\n\u001b[0;32m-> 1419\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concat_objects(\n\u001b[1;32m   1420\u001b[0m         values,\n\u001b[1;32m   1421\u001b[0m         not_indexed_same\u001b[39m=\u001b[39;49mnot_indexed_same,\n\u001b[1;32m   1422\u001b[0m         is_transform\u001b[39m=\u001b[39;49mis_transform,\n\u001b[1;32m   1423\u001b[0m     )\n\u001b[1;32m   1425\u001b[0m key_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mresult_index \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mas_index \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first_not_none, (np\u001b[39m.\u001b[39mndarray, Index)):\n\u001b[1;32m   1428\u001b[0m     \u001b[39m# GH#1738: values is list of arrays of unequal lengths\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m     \u001b[39m#  fall through to the outer else clause\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     \u001b[39m# TODO: sure this is right?  we used to do this\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m     \u001b[39m#  after raising AttributeError above\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1019\u001b[0m, in \u001b[0;36mGroupBy._concat_objects\u001b[0;34m(self, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     group_levels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mlevels\n\u001b[1;32m   1017\u001b[0m     group_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mnames\n\u001b[0;32m-> 1019\u001b[0m     result \u001b[39m=\u001b[39m concat(\n\u001b[1;32m   1020\u001b[0m         values,\n\u001b[1;32m   1021\u001b[0m         axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis,\n\u001b[1;32m   1022\u001b[0m         keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   1023\u001b[0m         levels\u001b[39m=\u001b[39;49mgroup_levels,\n\u001b[1;32m   1024\u001b[0m         names\u001b[39m=\u001b[39;49mgroup_names,\n\u001b[1;32m   1025\u001b[0m         sort\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1026\u001b[0m     )\n\u001b[1;32m   1027\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1028\u001b[0m     \u001b[39m# GH5610, returns a MI, with the first level being a\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[39m# range index\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m     keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(values)))\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/reshape/concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[1;32m    618\u001b[0m )\n\u001b[1;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/internals/concat.py:242\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    240\u001b[0m     fastpath \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m values\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    243\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/internals/concat.py:581\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, copy)\u001b[0m\n\u001b[1;32m    578\u001b[0m has_none_blocks \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(unit\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m unit \u001b[39min\u001b[39;00m join_units)\n\u001b[1;32m    579\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m--> 581\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[1;32m    582\u001b[0m     ju\u001b[39m.\u001b[39mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39mempty_dtype, upcasted_na\u001b[39m=\u001b[39mupcasted_na)\n\u001b[1;32m    583\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[1;32m    584\u001b[0m ]\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    587\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     concat_values \u001b[39m=\u001b[39m to_concat[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/internals/concat.py:582\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    578\u001b[0m has_none_blocks \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(unit\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m unit \u001b[39min\u001b[39;00m join_units)\n\u001b[1;32m    579\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m    581\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 582\u001b[0m     ju\u001b[39m.\u001b[39;49mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39;49mempty_dtype, upcasted_na\u001b[39m=\u001b[39;49mupcasted_na)\n\u001b[1;32m    583\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[1;32m    584\u001b[0m ]\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    587\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     concat_values \u001b[39m=\u001b[39m to_concat[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/internals/concat.py:489\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    487\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39m(isna_all(row) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m values)\n\u001b[0;32m--> 489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_reindexed_values\u001b[39m(\u001b[39mself\u001b[39m, empty_dtype: DtypeObj, upcasted_na) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m    490\u001b[0m     values: ArrayLike\n\u001b[1;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m upcasted_na \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    493\u001b[0m         \u001b[39m# No upcasting is necessary\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_is_stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_Thread__stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--------------------------#\n",
    "# 计算调试开关\n",
    "is_offline = True \n",
    "#--------------------------#\n",
    "\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435 \n",
    "lgb_accelerator = 'cuda' if is_offline else 'gpu'\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('mylogger')\n",
    "\n",
    "if is_offline:\n",
    "    data_path =r'/usr/src/kaggle_/optiver-trading-at-the-close'\n",
    "else:\n",
    "    data_path =r'/usr/src/kaggle_/optiver-trading-at-the-close'\n",
    "    # data_path = r'/kaggle/input/optiver-trading-at-the-close'\n",
    "path_train  = data_path+  '/train.csv'\n",
    "df_train = pd.read_csv(path_train)\n",
    "\n",
    "#  生成股票的子预测\n",
    "@njit(parallel = True)\n",
    "def calculate_stock_return(wap):\n",
    "    stock_return = np.zeros_like(wap)\n",
    "    for i in prange(len(wap)):\n",
    "        stock_return[i] = (np.log(wap[i] / wap[i - 6]) * 10_000) if i >= 6 else 0\n",
    "    return stock_return\n",
    "df_train = df_train.groupby([\"stock_id\", \"date_id\"]).apply(lambda frame: frame.assign(\n",
    "    stock_return=calculate_stock_return(frame[\"wap\"].values),\n",
    ")).reset_index(drop=True)\n",
    "df_train['index_return']=df_train[\"stock_return\"] - df_train[\"target\"]\n",
    "\n",
    "df_train = df_train.dropna(subset= ['index_return'])\n",
    "print(\"stocks returns generate finished!.\")\n",
    "df = df_train.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape\n",
    "print('Data Loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature function Loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_features(df):\n",
    "\n",
    "    features = ['seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "               'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "                'reference_price','far_price', 'near_price', 'ask_price', \n",
    "                'bid_price', 'wap','imb_s1', 'imb_s2']\n",
    "    \n",
    "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)')\n",
    "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)')\n",
    "    \n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            if i>j:\n",
    "                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})')\n",
    "                features.append(f'{a}_{b}_imb')\n",
    "                    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            for k,c in enumerate(prices):\n",
    "                if i>j and j>k:\n",
    "                    max_ = df[[a,b,c]].max(axis=1)\n",
    "                    min_ = df[[a,b,c]].min(axis=1)\n",
    "                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_\n",
    "                    df[f'{a}_{b}_{c}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "                    features.append(f'{a}_{b}_{c}_imb2')\n",
    "    \n",
    "    return df[features]\n",
    "\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max: \n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 7]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 7]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    # V4 \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "\n",
    "    # V5\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 7]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "    return df\n",
    "\n",
    "def rolling_features(df,):\n",
    "    window_size = [2, 3, 5, 7]\n",
    "    # F_rolling\n",
    "    rolling_features = ['bid_size', 'ask_size', 'bid_price', 'ask_price', 'imbalance_size', 'matched_size', 'wap']\n",
    "\n",
    "    for window_size_i in window_size:\n",
    "        for feature in rolling_features:\n",
    "            df[f'{feature}_rolling_std_{window_size_i}'] = df.groupby('stock_id')[feature].transform(lambda x: x.rolling(window=window_size_i, min_periods=1).std())\n",
    "            df[f'{feature}_rolling_median_{window_size_i}'] = df.groupby('stock_id')[feature].transform(lambda x: x.rolling(window=window_size_i, min_periods=1).median())\n",
    "    return df\n",
    "\n",
    "def relativedelta_features(df):\n",
    "    # F_expanding calc_relative_delta\n",
    "    window_size = [2, 3, 5, 7]\n",
    "    rolling_features = ['bid_size', 'ask_size', 'bid_price', 'ask_price', 'imbalance_size', 'matched_size', 'wap']\n",
    "    for window_size_i in window_size:\n",
    "        for feature in rolling_features:\n",
    "            denominator_ = df['mid_price'].expanding(window_size_i).max() - df['mid_price'].expanding(window_size_i).min()\n",
    "            df[f'{feature}_relativedelta_{window_size_i}_upside']  = ( df['mid_price'].expanding(window_size_i).max() - df['mid_price'])/denominator_\n",
    "            df[f'{feature}_relativedelta_{window_size_i}_downside'] = ( df['mid_price'] - df['mid_price'].expanding(window_size_i).min())/denominator_\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_TA_features(df):\n",
    "\n",
    "    @njit(parallel = True)\n",
    "    def calculate_rsi(prices, period=14):\n",
    "        rsi_values = np.zeros_like(prices)\n",
    "\n",
    "        for col in prange(prices.shape[1]):\n",
    "            price_data = prices[:, col]\n",
    "            delta = np.zeros_like(price_data)\n",
    "            delta[1:] = price_data[1:] - price_data[:-1]\n",
    "            gain = np.where(delta > 0, delta, 0)\n",
    "            loss = np.where(delta < 0, -delta, 0)\n",
    "\n",
    "            avg_gain = np.mean(gain[:period])\n",
    "            avg_loss = np.mean(loss[:period])\n",
    "            \n",
    "            if avg_loss != 0:\n",
    "                rs = avg_gain / avg_loss\n",
    "            else:\n",
    "                rs = 1e-9  # or any other appropriate default value\n",
    "                \n",
    "            rsi_values[:period, col] = 100 - (100 / (1 + rs))\n",
    "\n",
    "            for i in prange(period-1, len(price_data)-1):\n",
    "                avg_gain = (avg_gain * (period - 1) + gain[i]) / period\n",
    "                avg_loss = (avg_loss * (period - 1) + loss[i]) / period\n",
    "                if avg_loss != 0:\n",
    "                    rs = avg_gain / avg_loss\n",
    "                else:\n",
    "                    rs = 1e-9  # or any other appropriate default value\n",
    "                rsi_values[i+1, col] = 100 - (100 / (1 + rs))\n",
    "        return rsi_values\n",
    "    \n",
    "    @njit(parallel=True)\n",
    "    def calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "        rows, cols = data.shape\n",
    "        macd_values = np.empty((rows, cols))\n",
    "        signal_line_values = np.empty((rows, cols))\n",
    "        histogram_values = np.empty((rows, cols))\n",
    "\n",
    "        for i in prange(cols):\n",
    "            short_ema = np.zeros(rows)\n",
    "            long_ema = np.zeros(rows)\n",
    "\n",
    "            for j in range(1, rows):\n",
    "                short_ema[j] = (data[j, i] - short_ema[j - 1]) * (2 / (short_window + 1)) + short_ema[j - 1]\n",
    "                long_ema[j] = (data[j, i] - long_ema[j - 1]) * (2 / (long_window + 1)) + long_ema[j - 1]\n",
    "\n",
    "            macd_values[:, i] = short_ema - long_ema\n",
    "\n",
    "            signal_line = np.zeros(rows)\n",
    "            for j in range(1, rows):\n",
    "                signal_line[j] = (macd_values[j, i] - signal_line[j - 1]) * (2 / (signal_window + 1)) + signal_line[j - 1]\n",
    "\n",
    "            signal_line_values[:, i] = signal_line\n",
    "            histogram_values[:, i] = macd_values[:, i] - signal_line\n",
    "\n",
    "        return macd_values, signal_line_values, histogram_values\n",
    "    \n",
    "    @njit(parallel=True)\n",
    "    def calculate_bband(data, window=20, num_std_dev=2):\n",
    "        num_rows, num_cols = data.shape\n",
    "        upper_bands = np.zeros_like(data)\n",
    "        lower_bands = np.zeros_like(data)\n",
    "        mid_bands = np.zeros_like(data)\n",
    "\n",
    "        for col in prange(num_cols):\n",
    "            for i in prange(window - 1, num_rows):\n",
    "                window_slice = data[i - window + 1 : i + 1, col]\n",
    "                mid_bands[i, col] = np.mean(window_slice)\n",
    "                std_dev = np.std(window_slice)\n",
    "                upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\n",
    "                lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\n",
    "\n",
    "        return upper_bands, mid_bands, lower_bands\n",
    "    \n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    \n",
    "    for stock_id, values in df.groupby(['stock_id'])[prices]:\n",
    "        # RSI\n",
    "        col_rsi = [f'rsi_{col}' for col in values.columns]\n",
    "        rsi_values = calculate_rsi(values.values)\n",
    "        df.loc[values.index, col_rsi] = rsi_values\n",
    "        gc.collect()\n",
    "        \n",
    "        # MACD\n",
    "        macd_values, signal_line_values, histogram_values = calculate_macd(values.values)\n",
    "        col_macd = [f'macd_{col}' for col in values.columns]\n",
    "        col_signal = [f'macd_sig_{col}' for col in values.columns]\n",
    "        col_hist = [f'macd_hist_{col}' for col in values.columns]\n",
    "        \n",
    "        df.loc[values.index, col_macd] = macd_values\n",
    "        df.loc[values.index, col_signal] = signal_line_values\n",
    "        df.loc[values.index, col_hist] = histogram_values\n",
    "        gc.collect()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bband_upper_values, bband_mid_values, bband_lower_values = calculate_bband(values.values, window=20, num_std_dev=2)\n",
    "        col_bband_upper = [f'bband_upper_{col}' for col in values.columns]\n",
    "        col_bband_mid = [f'bband_mid_{col}' for col in values.columns]\n",
    "        col_bband_lower = [f'bband_lower_{col}' for col in values.columns]\n",
    "        \n",
    "        df.loc[values.index, col_bband_upper] = bband_upper_values\n",
    "        df.loc[values.index, col_bband_mid] = bband_mid_values\n",
    "        df.loc[values.index, col_bband_lower] = bband_lower_values\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    df = other_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    df = rolling_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    df = relativedelta_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "    # df = add_TA_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    return df[feature_name]\n",
    "\n",
    "def select_features(df,method = 'corr',select_ratio = 0.75):\n",
    "\n",
    "    def pca_feature_selection(df, n_components):\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # 标准化特征矩阵\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(df.values)\n",
    "        # 创建PCA对象并拟合数据\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_selected = pca.fit_transform(X_scaled)\n",
    "        # 构造降维后的DataFrame\n",
    "        columns = [f\"Component_{i+1}\" for i in range(n_components)]\n",
    "        df_selected = pd.DataFrame(X_selected, columns=columns)\n",
    "        return df_selected\n",
    "    \n",
    "    def corr_feature_selection(df,n_components):\n",
    "\n",
    "        corr_se = df.corr().abs().sum()\n",
    "        correlated_features  = corr_se.sort_values().iloc[int(np.round(n_components)):].index\n",
    "        df_selected = df.drop(correlated_features,axis=1)\n",
    "        return df_selected\n",
    "    \n",
    "    select_ratio = 0.64\n",
    "\n",
    "    if method  == 'pca':\n",
    "        k = len(df.columns)*select_ratio\n",
    "        df = pca_feature_selection(df, k)\n",
    "        return df\n",
    "    \n",
    "    elif method == \"corr\":\n",
    "        k = len(df.columns)*select_ratio\n",
    "        df = corr_feature_selection(df, k)\n",
    "        return df\n",
    "    \n",
    "    elif method == 'no':\n",
    "        return df\n",
    "print('Feature function Loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline mode\n",
      "train : (4225417, 19), valid : (440951, 19)\n",
      "Build Train Feats Finished.\n",
      "Build Valid Feats Finished.\n",
      "Processing of all features in the dataframe (df) is completed!\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "])\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}\n",
    "\n",
    "if is_offline:\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")\n",
    "\n",
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "\n",
    "        df_valid_feats = select_features(df_valid_feats)\n",
    "        target_col = df_valid_feats.columns\n",
    "        df_train_feats = df_train_feats[target_col]\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        df_train_feats = select_features(df_train_feats)\n",
    "        target_col = df_train_feats.columns\n",
    "        df_train_feats = df_train_feats[target_col]\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "\n",
    "print('Processing of all features in the dataframe (df) is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Loaded!\n"
     ]
    }
   ],
   "source": [
    "model_dict_list = [\n",
    "    #             {\n",
    "    #     'model': lgb.LGBMRegressor,\n",
    "    #     'name': 'lgb',\n",
    "    #     \"params\":{\n",
    "    #     \"objective\": \"mae\",\n",
    "    #     \"n_estimators\": 6000,\n",
    "    #     \"num_leaves\": 256,\n",
    "    #     \"subsample\": 0.6,\n",
    "    #     \"colsample_bytree\": 0.8,\n",
    "    #     \"learning_rate\": 0.00871,\n",
    "    #     'max_depth': 11,\n",
    "    #     \"n_jobs\": 4,\n",
    "    #     \"device\": \"cuda\",\n",
    "    #     \"verbosity\": 1,\n",
    "    #     \"importance_type\": \"gain\",}\n",
    "    #     ,\n",
    "    #     \"callbacks\": [\n",
    "    #     lgb.callback.early_stopping(stopping_rounds=100),\n",
    "    #     lgb.callback.log_evaluation(period=100),\n",
    "    #     ]\n",
    "    # },\n",
    "\n",
    "    #     {\n",
    "    #     'model': lgb.LGBMRegressor,\n",
    "    #     'name': 'lgb',\n",
    "    #     \"params\":{\n",
    "    #     \"objective\": \"mae\",\n",
    "    #     \"n_estimators\": 6000,\n",
    "    #     \"num_leaves\": 256,\n",
    "    #     \"subsample\": 0.6,\n",
    "    #     \"colsample_bytree\": 0.8,\n",
    "    #     \"learning_rate\": 0.00871,\n",
    "    #     'max_depth': 11,\n",
    "    #     \"n_jobs\": 8,\n",
    "    #     \"device\": \"cuda\",\n",
    "    #     \"verbosity\": 1,\n",
    "    #     \"importance_type\": \"gain\",\n",
    "    #     \"min_child_samples\": 15,  # Minimum number of data points in a leaf\n",
    "    #     \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "    #     \"reg_lambda\": 0.3,  # L2 regularization term\n",
    "    #     \"min_split_gain\": 0.2,  # Minimum loss reduction required for further partitioning\n",
    "    #     \"min_child_weight\": 0.001,  # Minimum sum of instance weight (hessian) in a leaf\n",
    "    #     \"bagging_fraction\": 0.9,  # Fraction of data to be used for training each tree\n",
    "    #     \"bagging_freq\": 5,  # Frequency for bagging\n",
    "    #     \"feature_fraction\": 0.9,  # Fraction of features to be used for training each tree\n",
    "    #     \"num_threads\": 4,  # Number of threads for LightGBM to use\n",
    "    #     }\n",
    "    #     ,\n",
    "    #     \"callbacks\": [\n",
    "    #     lgb.callback.early_stopping(stopping_rounds=100),\n",
    "    #     lgb.callback.log_evaluation(period=100),\n",
    "    #     ]\n",
    "    # },\n",
    "\n",
    "\n",
    "    {\n",
    "        'model': cbt.CatBoostRegressor,\n",
    "        'name': 'cat',\n",
    "        'params': dict(iterations=2000,\n",
    "                       learning_rate=0.05,\n",
    "                       depth=12,\n",
    "                       l2_leaf_reg=30,\n",
    "                       bootstrap_type='Bernoulli',\n",
    "                       subsample=0.66,\n",
    "                       loss_function='MAE',\n",
    "                       eval_metric='MAE',\n",
    "                       metric_period=100,\n",
    "                       od_type='Iter',\n",
    "                       od_wait=30,\n",
    "                       task_type='GPU',\n",
    "                       allow_writing_files=False\n",
    "                       ),\n",
    "        'callbacks': []\n",
    "    }\n",
    "]\n",
    "print('Params Loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 152\n"
     ]
    }
   ],
   "source": [
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now,we are going to training!~\n"
     ]
    }
   ],
   "source": [
    "print('Now,we are going to training!~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now model is cat\n",
      "Feature length = 152\n",
      "Valid Model Trainning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3033dd091240a6bb2a5feec40ce8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1 out of 3\n",
      "0:\tlearn: 7.8262533\ttest: 7.1606446\tbest: 7.1606446 (0)\ttotal: 35.2ms\tremaining: 1m 10s\n",
      "100:\tlearn: 5.6892858\ttest: 5.0579830\tbest: 5.0579830 (100)\ttotal: 3.33s\tremaining: 1m 2s\n",
      "200:\tlearn: 4.1200145\ttest: 3.5548875\tbest: 3.5548875 (200)\ttotal: 6.39s\tremaining: 57.2s\n",
      "300:\tlearn: 2.9934928\ttest: 2.5079768\tbest: 2.5079768 (300)\ttotal: 9.74s\tremaining: 55s\n",
      "400:\tlearn: 2.1965677\ttest: 1.7912426\tbest: 1.7912426 (400)\ttotal: 13.3s\tremaining: 52.9s\n",
      "500:\tlearn: 1.6344630\ttest: 1.3018790\tbest: 1.3018790 (500)\ttotal: 17.1s\tremaining: 51.1s\n",
      "600:\tlearn: 1.2364067\ttest: 0.9674159\tbest: 0.9674159 (600)\ttotal: 21s\tremaining: 48.8s\n",
      "700:\tlearn: 0.9547844\ttest: 0.7381455\tbest: 0.7381455 (700)\ttotal: 24.9s\tremaining: 46.2s\n"
     ]
    }
   ],
   "source": [
    "for model_dict in model_dict_list:\n",
    "\n",
    "    name = model_dict['name']\n",
    "    print(f'now model is {name}')\n",
    "    model_ = model_dict['model']\n",
    "    model_params = model_dict['params']\n",
    "    call_back_func  = model_dict['callbacks']\n",
    "    if is_train:\n",
    "        feature_name = list(df_train_feats.columns)\n",
    "        print(f\"Feature length = {len(feature_name)}\")\n",
    "        stock_id_arr = df.stock_id.values\n",
    "        offline_split = df_train['date_id']>(split_day - 45)\n",
    "        df_offline_train = df_train_feats[~offline_split].copy(deep = True)\n",
    "        df_offline_valid = df_train_feats[offline_split].copy(deep = True)\n",
    "        df_offline_train_target = df_train['stock_return'][~offline_split].copy(deep = True)\n",
    "        df_offline_valid_target = df_train['stock_return'][offline_split].copy(deep = True)\n",
    "\n",
    "        print(\"Valid Model Trainning.\")\n",
    "        _model = model_(**model_params)\n",
    "        if name == 'lgb':\n",
    "            _model.fit(\n",
    "                df_offline_train[feature_name],\n",
    "                df_offline_train_target,\n",
    "                eval_set=[( df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "                callbacks = call_back_func \n",
    "            )\n",
    "        elif name == 'cat':\n",
    "                summary = _model.select_features(\n",
    "                            df_offline_train[feature_name], df_offline_train_target,\n",
    "                            eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "                            features_for_select=feature_name,\n",
    "                            num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n",
    "                            steps=3,\n",
    "                            algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n",
    "                            shap_calc_type=EShapCalcType.Regular,\n",
    "                            train_final_model=False,\n",
    "                            plot=True,\n",
    "                        )\n",
    "                _model.fit(\n",
    "                        df_offline_train[summary['selected_features_names']], df_offline_train_target,\n",
    "                        eval_set=[(df_offline_valid[summary['selected_features_names']], df_offline_valid_target)],\n",
    "                        use_best_model=True,\n",
    "                    )\n",
    "        else:\n",
    "            _model.fit(\n",
    "                df_offline_train[feature_name],\n",
    "                df_offline_train_target,\n",
    "                eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "            )\n",
    "        \n",
    "        del df_offline_train, df_offline_train_target\n",
    "        gc.collect()\n",
    "\n",
    "        # infer\n",
    "        df_train_target = df_train['stock_return']\n",
    "        print(\"Infer Model Trainning.\")\n",
    "        infer_params = model_params.copy()\n",
    "        best_iter_n = _model.best_iteration_ if hasattr(_model, \"best_iteration_\") else _model.best_iteration\n",
    "        infer_params[\"n_estimators\"] = int(1.2 * best_iter_n)\n",
    "        infer__model =  model_(**model_params)\n",
    "        if name == 'lgb':\n",
    "            infer__model.fit(df_train_feats[feature_name],\n",
    "                            df_train_target,\n",
    "                            eval_set=[(df_offline_valid[feature_name],\n",
    "                            df_offline_valid_target)],\n",
    "                            callbacks = call_back_func \n",
    "                            )\n",
    "        elif name == 'cat':\n",
    "            infer__model.fit(df_train_feats[summary['selected_features_names']], df_train_target)\n",
    "        else:\n",
    "            infer__model.fit(df_train_feats[feature_name],\n",
    "                            df_train_target,\n",
    "                            eval_set=[(df_offline_valid[feature_name],\n",
    "                            df_offline_valid_target)],\n",
    "                            )\n",
    "\n",
    "        if is_offline:   \n",
    "            # offline predictions\n",
    "            df_valid_target = df_valid['stock_return']\n",
    "            apd_index  = df_valid['index_return']\n",
    "            offline_predictions = infer__model.predict(df_valid_feats[feature_name])\n",
    "            weighted_ = df_valid.stock_id.map(weights).values\n",
    "            offline_score = mean_absolute_error(offline_predictions*weighted_, df_valid_target)\n",
    "            print(f\"Offline Score {np.round(offline_score, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
